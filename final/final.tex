%for a more compact document, add the option openany to avoid
%starting all chapters on odd numbered pages
\documentclass[12pt]{cmuthesis}

% This is a template for a CMU thesis.  It is 18 pages without any content :-)
% The source for this is pulled from a variety of sources and people.
% Here's a partial list of people who may or may have not contributed:
%
%        bnoble   = Brian Noble
%        caruana  = Rich Caruana
%        colohan  = Chris Colohan
%        jab      = Justin Boyan
%        josullvn = Joseph O'Sullivan
%        jrs      = Jonathan Shewchuk
%        kosak    = Corey Kosak
%        mjz      = Matt Zekauskas (mattz@cs)
%        pdinda   = Peter Dinda
%        pfr      = Patrick Riley
%        dkoes = David Koes (me)

% My main contribution is putting everything into a single class files and small
% template since I prefer this to some complicated sprawling directory tree with
% makefiles.

% some useful packages
\usepackage{times}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{multirow}
\usepackage[numbers,sort]{natbib}
\usepackage[pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
%pdfborder=0 0 0,  %removes outlines around hyper links in online display
]{hyperref}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{titlesec}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
%\newcommand\addtag{\refstepcounter{equation}\tag{\theequation}}
\titleformat{\chapter}[hang] 
{\normalfont\LARGE\bfseries}{\chaptertitlename\ \thechapter:}{1em}{}

% Approximately 1" margins, more space on binding side
%\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar]{geometry}
%for general printing (not binding)
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

% Provides a draft mark at the top of the document. 
\draftstamp{\today}{DRAFT}

\begin {document} 
\frontmatter

%initialize page style, so contents come out right (see bot) -mjz
\pagestyle{empty}

\title{ %% {\it \huge Thesis Proposal}\\
{\bf Guitar Fretboard-Position Classification using Inharmonicity Regression and Maximum-likelihood Estimation}}
\author{Jonathan Michelson}
\date{June 2017}
\Year{2017}
\trnumber{}

\committee{
Dr. Richard Stern, Dr. Thomas Sullivan \\
}

\support{}
\disclaimer{}

% copyright notice generated automatically from Year and author.
% permission added if \permission{} given.

%\keywords{Stuff, More Stuff}

\maketitle

%\begin{dedication}
%For my dog
%\end{dedication}

\pagestyle{plain} % for toc, was empty

%% Obviously, it's probably a good idea to break the various sections of your thesis
%% into different files and input them into this file...

%\doublespacing
\begin{abstract}
We propose and evaluate two new methods to classify guitar strings with the goal of developing automated tablature transcription. The classifiers use only audio input and are based on the measured inharmonicity of the sounds produced by the strings. While previous inharmonicity-based systems were able to obtain good performance classifying guitar strings based on sample data produced by individual guitars, we attempt to develop a system that can obtain improved string-identification performance for guitars on which the system wasn't individually trained, using training data aggregated from multiple guitars of the same type. We also derive and evaluate an inharmonicity compensation feature to allow for tablature transcription of arbitrarily-tuned guitars, provided the tuning is known. 

The first method characterizes each guitar string as the linear trajectory of its log-inharmonicity with respect to its fundamental pitch, then assigns unseen notes to the strings whose trajectories best approximate the notes' estimated inharmonicities. The second method characterizes the inharmonicity distributions of each fretboard position as normal probability densities, then assigns unseen notes to the fretboard positions that maximize the likelihood of having observed their estimated inharmonicities. Results from classical, acoustic, and electric guitars in the standard Real World Corpus of guitar recordings show that both methods perform better on more guitars than the existing inharmonicity-based system for test recordings on which the system wasn't individually trained. Also, results from personally-recorded acoustic and electric guitars in various alternate tunings show that our tuning compensation is successful at predicting tuning-related inharmonicity changes and improving transcription performance.

\end{abstract}
%\singlespacing

\begin{acknowledgments}
My sincere gratitude goes to Rich Stern for advising me every step of this journey. I'd never before tackled a research project of this scope, but Rich's guidance, patience, and insight somehow made this thesis happen. It's been an immeasurably rewarding experience, and I owe that to him. I'll miss our weekly one-on-one meetings that evolved into friendly chats about music, family, and life.

I'm indebted to Tom Sullivan for lending his qualified examination of this thesis. I'm also indebted to him in the literal sense as he welcomed me to Pittsburgh by treating me to lunch. His hospitality and accessibility helped me know I was in the right place.

I'd like to thank Matthieu Hodgkinson for sharing with me his MAT algorithm code, as well as Jacob Abesser and Isabel Barbancho for their helpful correspondence. 

My peers Che-yuan Liang and Raymond Xia have both, of their own accord, contributed meaningfully to this work through our many brainstorming and discussion sessions, and for them I'm grateful. Many thanks to Nick Pourazima, whose Fender Telecaster we recorded for experiments.

I need to thank the remainder of my CMU graduate music technology cohort -- Devansh Zurale, Steven Krenn, Chen Liang, Mutian Fu, Hao Huang, Anirudh Mani, Garrett Osborn, and Nick Pourazima -- for the support role they played by simply being the fun constructive community that they were.

Finally, I acknowledge my mom and dad, to whom all success is due. I'd also like to thank my brother Nick Michelson solely for persuading me to abandon writing a grandiose Acknowledgments section, and definitely nothing else.

\end{acknowledgments}



\tableofcontents
\listoffigures
\listoftables

\mainmatter

%% Double space document for easy review:
%\renewcommand{\baselinestretch}{1.66}\normalsize

% The other requirements Catherine has:
%
%  - avoid large margins.  She wants the thesis to use fewer pages, 
%    especially if it requires colour printing.
%
%  - The thesis should be formatted for double-sided printing.  This
%    means that all chapters, acknowledgements, table of contents, etc.
%    should start on odd numbered (right facing) pages.
%
%  - You need to use the department standard tech report title page.  I
%    have tried to ensure that the title page here conforms to this
%    standard.
%
%  - Use a nice serif font, such as Times Roman.  Sans serif looks bad.
%
% Other than that, just make it look good...
\doublespacing
\noindent
\chapter{Introduction} 
The guitar is a popular musical instrument whose family comprises a diverse collection of stringed instruments. Its most prominent classes are the acoustic, classical, and electric guitars, and intraclass variation is high in characteristics such as body shape, string material, and amplification modality. Despite this diversity, the majority of guitar configurations exhibit some unifying features, like having six strings tuned to $E2$ ($82$Hz), $A2$ ($110$Hz), $D3$ ($147$Hz), $G3$ ($196$Hz), $B3$ ($247$Hz), and $E4$ ($330$Hz), for example. Performance is relatively constant as well; to play, a performer presses these strings with one hand against the neck's metal frets, each of which increase the string's pitch by one half-step, while strumming or plucking them with the other hand. Guitars typically have upwards of 20 frets, allowing for versatile musical passage realizations along the fretboard.

A consequence of the strings' open-tunings and extensive fretting locations is liberal pitch overlap between neighboring strings; multiple fretboard locations can usually be selected to produce a given pitch. Conventional music scores that represent passages as notes and chords therefore fail to communicate fretboard position. This ambiguity renders scores unfit for guitar students trying to learn more skillful placement of scales and arpeggios, or for enthusiasts trying to decrypt the fretboard positions used on recordings of virtuosic players' riffs. Figure~\ref{fig:score-tabs} illustrates this ambiguity.

Tablature, on the other hand, is an alternative music notation that doesn't suffer from the one-to-many mapping of scores. The staff, instead of representing pitch as in classical scores, depicts a birds-eye view of the guitar neck from the performer's perspective. Each of the six horizontal lines signify a corresponding string on the guitar, and numbers on each line specify the fret to be played. Time progresses from left to right as in scores. 

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.25]{score-tabs}
\caption{Musical score and two identical realizations of that score in tablature notation. Figure taken from~\cite{barbanchoi2012}.}
\label{fig:score-tabs}
\end{figure}

Tablature (abbr. tabs) is widely more popular than scores for a number of likely reasons~\cite{macrae2010}. Its accessibility is attractive to those without formal musical training; interpreting a pictorial representation of the fretboard poses a smaller learning curve than studying music theory and deciphering score notation. Additionally, the lines and numbers in tablature that represent the strings and frets are easily represented with ASCII characters, rendering it an exceptionally portable mode of communication. Indeed, one can create, modify, and transfer tablature files using simply a text editor.

Accurate tablature transcription is a fairly laborious task, however. Dependable tabs containing few errors typically require a seasoned musician's careful listening to an audio recording, and possibly video recordings for additional consultation too. Reliable automation of this transcription task would benefit the guitar student community by expediting quality tab generation and unearthing fretboard positions of guitar riffs in songs for which accurate tabs don't yet exist. Recent technologies proposed to encourage proliferation of accessible tab transcription systems, such as web framework Robotaba~\cite{burlet2013}, demonstrate a desire among MIR researchers to see this theoretically mature but practically nascent field mature into student-benefitting applications. 

An important feature in systems that transcribe tablature from audio input is inharmonicity. Briefly, inharmonicity is a phenomenon that skews a vibrating string's harmonic partials upwards in frequency and whose variation trend is somewhat unique per string. Because of this, it is highly discriminative and has successfully been used as the sole feature in a transcription system~\cite{barbanchoi2012} which we refer to as the existing inharmonicity-based method. This is discussed further in Chapter~\ref{chap:lit-review}, but for now its introduced simply for context.

This work builds on the existing method. We propose an alternative procedure to exploit the inharmonicity of an unseen note for string classification: we assign to it the string whose pre-computed log-inharmonicity trajectory predicts the note's estimated log-inharmonicity with minimal error. To contextualize its performance as well as that of its predecessor, we also introduce a baseline inharmonicity-based classifier. To the author's knowledge, no results for strictly inharmonicity-based systems have yet been reported from a simple classifier for the sake of benchmarking. We therefore also propose a maximum-likelihood estimator of fretboard position given an unseen note. The measured inharmonicities of each fretboard position are collected and fit to a normal distribution, and are used to return the string that maximizes the probability of having observed the unseen note's measured inharmonicity.

We evaluate these two systems' performances in three scenarios, comparing their performance to that of the existing method in~\cite{barbanchoi2012}: when the systems have been trained on recordings of only the test guitar, when the systems have been trained on recordings of multiple guitars including the test guitar (all of which were in the same classical, acoustic, or electric class), and when the systems have been trained and tested on separate sets of guitars. Results show that although the existing system excels in the first case, these new systems succeed in the second (and presumably the third, though no results for the existing method were reported in~\cite{barbanchoi2012}) case, suggesting these methods' greater generalizability. 

Additionally, this work introduces an inharmonicity compensation factor that allows for successful transcription of guitars whose tunings differ from those of the training set, provided that the alternate-tunings are known. Experiments on a personally recorded Fender Telecaster and a Takamine G-series in various tunings reveal that our compensation factor can restore an arbitrarily-tuned guitar's transcription performance to that of its more accurately transcribed standard-tuned trial.

In the next chapter, we survey previous work in the field of automatic tablature transcription, highlighting the existing inharmonicity-based method. This is followed by a more substantial motivation of inharmonicity's suitability for the string discrimination task and a review of the literature on inharmonicity estimation. In Chapter~\ref{chap:method}, we introduce both systems in detail. Chapter~\ref{chap:experiments} reports the results of our experiments, and Chapter~\ref{chap:discussion} contains analysis of the results, limitations of significance, and future work. We summarize this work in Chapter~\ref{chap:conclusion}.

\noindent
\chapter{Previous Work}
\label{chap:lit-review}
Music transcription is a rich research topic with many contributors. We begin with a brief review of various teams who have focused specifically on tablature transcription. We then introduce a feature of guitar audio referred to as inharmonicity, which has been used exclusively and successfully in one system, whose detailed summary we defer to Chapter~\ref{chap:method}. We close this chapter with a survey of the inharmonicity estimation literature.

\section{Automatic Tablature Transcription}
\subsection{Graph Search Methods}
A popular framework for the stringed-instrument transcription task, introduced by Sayegh~\cite{sayegh1989} for both transcription and fingering problems, is that of weighted directed graphs that capture all plausible sequences of fretboard positions for a given musical passage. Edge weights in the graph are typically informed by mechanical or musical desirability of the fretboard sequences implied by the vertex transitions. Radicioni~\cite{radicioni2005} used this graph paradigm to obtain optimal fingerings from music scores. Yazawa~\cite{yazawa2013} extracted pitch and melody information from guitar recordings, and computed an optimal fingering based on plausible multipitch estimation results. Burlet~\cite{burlet2013} developed a polyphonic extension to monophonic optimal search algorithms. Burlet~\cite{burlet2015} also introduced a guitar-specific deep belief network whose polyphonic transcription output was arranged into tablature with a graph search routine. Radisavljevic~\cite{radisav2004} proposed a method to learn the optimal weights on the graph given optimal tablature.

\subsection{Machine Learning Solutions}
A number of teams have used machine learning techniques to ascertain fretboard position or related parameters. Gagnon~\cite{gagnon2003} proposed using neural networks on scores to deduce general fretboard region position and number of strings playing, while Tuohy~\cite{tuohy2006} used a genetic algorithm on scores to obtain tablature. Barbancho~\cite{barbancho2009} attempted guitar string classification with a Fisher linear discriminant using a multitude of time and frequency-domain features. Barbancho~\cite{barbanchoa2012} used HMMs on polyphonic recordings of common guitar chord sequences to transcribe their fret position and fingering. Abesser~\cite{abesser2012}, Dittmar~\cite{dittmar2013}, and Kehling~\cite{kehling2014} achieve good string transcription performance after training an SVM on standard-tuning guitar audio and using a slew of partials-related features.

\subsection{Multimedia Approaches}
Some researchers have explored usage of media other than audio or musical information (i.e., scores). O'grady~\cite{ogrady2009} exploited hexaphonic pickups and matrix factorization of their output to record fretboard positions. Paleari~\cite{paleari2008}, Burns~\cite{burns2006}, and Kerdvibulvich~\cite{kerd2007} extract tablature from video recordings using image processing and computer vision techniques.

\subsection{The Partial Coincidence Tally (PCT) Method}
One system in particular served as inspiration for this work. Barbancho~\cite{barbanchoi2012} realized the discriminative potential of a singular feature known as inharmonicity, and devised an algorithm that exclusively used inharmonicity to achieve impressive transcription results using only audio. We discuss inharmonicity in detail and methods for its estimation in the remainder of this chapter. Their method, which we refer to as the partial coincidence tally (PCT) method, is also given a more thorough treatment later in Chapter~\ref{chap:method}.

We were particularly interested in the PCT method for a number of reasons. Firstly, its input is audio. Among the most interesting use cases for an automatic tab transcription system are direct audio-to-tablature capability, as audio is the natural output of the object we're interested in transcribing. Other inputs, like scores or video, are already an extent of transcription that detracts from the impressiveness of a tablature transcriber. Secondly, because PCT examines notes' underlying physical signatures, it can theoretically decode the fretboard position of any note despite how unexpected its location may be. Other methods, like graph searches, rely on optimization of cost functions that are generalizations of tab playability rules, and therefore aren't as well-suited to correctly transcribe aberrant note sequences. Lastly, their method exclusively uses inharmonicity. Other systems that use inharmonicity~\cite{barbancho2009,abesser2012,dittmar2013,kehling2014} exploit additional features that aid in string discriminability. We were captivated by the idea of singularly using inharmonicity to achieve impressive transcription scores.

\section{Inharmonicity Motivation}
To understand the motivation for this work, we first need to address the motivation behind inharmonicity as a feature itself. Inharmonicity is key in many string discrimination systems~\cite{barbancho2009,barbanchoi2012,abesser2012,dittmar2013,kehling2014}. When an ideal string fixed at both ends is displaced, the restoring force that causes it to oscillate is its tension. However, for a real string with stiffness, its elasticity contributes to this restoring force~\cite{fletcher1962} which causes the frequencies of the modes of vibration to no longer be integer multiples of the fundamental. Instead, they're skewed upward according to: 
\begin{equation}
\label{eq:fk}
f_k = kf_{0}\sqrt{1+\beta k^2}
\end{equation}
where $f_k$ is the $k$th harmonic of fundamental $f_0$ and $\beta$ is the string's inharmonicity, defined by
\begin{equation}
\beta = \frac{\pi^3 Q d^4}{64 T l^2}. \label{eq:beta}
\end{equation}
In words, the inharmonicity $\beta$ of a vibrating string is a dimensionless quantity that depends on the string's Young's modulus $Q$, diameter $d$, tension $T$, and vibrating length $l$, and which scales the degree of deviation of the string's $k$th partial according to equation~(\ref{eq:fk}). See Figure~\ref{fig:skew}. Note that for an ideally harmonic string, $\beta = 0$ and~(\ref{eq:fk}) reduces to $f_k = kf_0$, aligning with intuition about harmonics' ideal locations in frequency as simply integer multiples of the fundamental.

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.65]{skew}
\caption{Power spectrum of note D2 plucked on an electric guitar at string 5 and fret 5. Red dashed lines are drawn at integer multiples of the fundamental. Note the rightward skew of the measured partial peaks.}
\label{fig:skew}
\end{figure}

Inharmonicity's suitability for the string discrimination task can be understood through equation~(\ref{eq:beta}). The six strings of any guitar necessarily exhibit different combinations of $Q$, $d$, and $T$ due to varying thicknesses, material, and tuning. Each string therefore has an ``inharmonic signature" which presumably distinguishes it from the others. Capturing the pattern of a string's inharmonicity variation along its frets should thus reveal sufficiently identifying information.

\section{Inharmonicity Estimation}
\label{lit-beta-est}
Empirical estimation of inharmonicity is itself a topic of research which merits a brief review. Using pitch extraction techniques to perform estimation was first attempted by Galembo in 1979 and 1986~\cite{galembo1979,galembo1987}. Several additional methods have been proposed since then. In 1994, Galembo~\cite{galembo1994} hypothesized a connection between partials-based fundamental pitch estimates and the degree of inharmonicity in a spectrum. They specifically investigated cepstral analysis and a variant of the harmonic product spectrum method applied to both synthetic tones and recorded piano notes. They found that, because these two techniques leverage periodicity in the frequency domain to produce their pitch estimates, inharmonicity influenced the quality of their estimates in a deterministic manner: more inharmonicity produced wider and less focused pitch estimate lobes, and from quantifying this relation an inharmonicity estimate could be obtained.
 
 The same authors in~\cite{galembo1999} introduced another method in 1999, dubbed the inharmonic comb filter (ICF) method. To estimate a note's inharmonicity, they first applied sets of comb filters to the note's spectrum. The locations of the comb filters' notches were parametrized with a range of inharmonicity values that sufficiently sampled the note's expected inharmonicity range. The ICF whose inharmonicity best matched that of the note would produce the output with lowest spectral energy, since its notches would most align with the inharmonic note partials. The inharmonicity parameter associated with this output-minimizing ICF was thus selected as the estimate.
 
Rauhala~\cite{rauhala2007} introduced an efficient procedure in 2007 that iteratively catalogued estimates of the partial frequencies' deviations and returned increasingly better estimates of the inharmonicity. The algorithm, known as the partial frequency deviations (PDF) method, is initialized with a reasonable first estimate of inharmonicity, and then the note's spectrum is searched for partial peaks. Differences are measured between the locations of these discovered peaks and those of the expected peaks, and the aggregate deviation trend is used to refine the inharmonicity estimate -- a majority of positive differences implies the inharmonicity should be reduced, and a majority of negative differences implies the opposite.

In 2009, Hodgkinson~\cite{hodgkinson2009} proposed a yet more efficient and accurate algorithm, named median-adjustive trajectories (MAT). Their routine exploits the fact that inharmonicity can be estimated with any two partials and their corresponding indices in the harmonic series. Pairs of low-index partials are first considered, since they're minimally affected by inharmonic skew and therefore have high location reliability. With these initial inharmonicity estimates, additional partials are collected and used to refine the previous inharmonicity estimates, with which more partials are located, so on and so forth. MAT was the inharmonicity estimation routine employed in this work, and is discussed at length in Chapter~\ref{chap:method}.

Three years later, Barbancho~\cite{barbanchoi2012} introduced another PFD-based approach as part of a guitar tablature transcription system. They started by cataloguing the first 10 partials' deviations, then fitting a polynomial to the PFD curve. They derived the relation between the polynomial's coefficients and the inharmonicity, and returned an initial inharmonicity estimate according to this derivation. Then using this inharmonicity to aid in their search for higher-index partials, they catalogued a larger number of partials' deviations than that of the first iteration, and performed the same polynomial fit and inharmonicity derivation routine. This would refine the inharmonicity estimate, allowing for increasing numbers of partials for which to search.

Also in 2012, Abesser~\cite{abesser2012} used parametric spectral modeling, again in the context of a guitar tablature transcription system. To estimate the inharmonicity of a plucked guitar note, they modeled each audio frame with the autoregressive (AR) filter that best approximated its spectral content. They obtained the poles' locations and the ideal harmonics' locations, and like Barnbancho~\cite{barbanchoi2012} fit the PFD curve with a polynomial from whose coefficients an inharmonicity estimate could be derived.



\noindent
\chapter{Proposed Inharmonicity-Based Transcription Methods}
\label{chap:method}
In this chapter, we present both our inharmonicity trajectory and baseline tablature transcription systems in detail. More time was spent innovating and developing the regression-based classifier, so we appropriately give it a lengthier treatment. We begin with its motivation, then present its various building blocks: the specific inharmonicity estimation algorithm we used, transformation of inharmonicity to log-inharmonicity, learning log-inharmonicity trajectories, string classification using the learned trajectories, and the final tablature conversion and refinement process. We then discuss the baseline classifier's motivation, its inharmonicity estimation algorithm, and its fretboard position classification mechanism. We next introduce our feature that allows compensation for arbitrary tuning before concluding with overviews of the two presented.

\section{Characterizing Strings as Inharmonicity Trajectories}
\subsection{Motivation}
Barbancho~\cite{barbanchoi2012} derived that inharmonicity varies in a deterministic manner with respect to fret positions along a given string. Specifically, they showed that the inharmonicity $\beta(s,n)$ of a note produced by string $s$ at fret number $n$ could be expressed in terms of the inharmonicity of the open-string note $\beta(s,0)$ according to:
\begin{equation} 
\label{eq:beta-traj}
\beta(s,n) = \beta(s,0)2^{\frac{n}{6}}
\end{equation}
In other words, the variation in inharmonicity along a given string can be expressed simply in terms of its open-note inharmonicity, effectively defining a restricted trajectory along which the inharmonicity must vary. Figure~\ref{fig:beta-trajectories-ag} illustrates these trajectories.

\begin{figure}[h] 
\centering
\includegraphics[scale=0.70]{beta-trajectories-ag}
\caption{Example inharmonicity trajectories of acoustic guitar strings, calculated from measured open-string inharmonicities. Only frets 0 (open-string) through 3 are shown. Observe the restriction to the exponential trajectory defined by equation~\eqref{eq:beta-traj}.}
\label{fig:beta-trajectories-ag}
\end{figure}

Recognizing this, they were able to successfully transcribe string and fret number of an unknown guitar note. Their system's essential mechanism was a similarity-determination routine $A(X,\hat{\beta}_s)$ that quantified the resemblance between an unknown note's spectrum $X$ and the expected inharmonicities $\hat\beta_s$ of candidate strings $s$ from which the note could have originated. First, to estimate an open-note's inharmonicity they followed the procedure described in section~\ref{lit-beta-est} in which they estimated its fundamental, then catalogued its partials' deviations from their ideal harmonic locations, and fit to them a polynomial from whose coefficients they derived the inharmonicity estimate. Next, they inferred the corresponding fret on which the note could be played for each candidate string, and used these string-fret candidate numbers in equation~\eqref{eq:beta-traj} to obtain expected inharmonicity values of the unknown note for each candidate string. In other words, they followed each candidate string's theoretical inharmonicity trajectory to arrive at an expected inharmonicity value at the appropriate fret. Lastly, they applied their similarity determination algorithm $A(X,\hat{\beta}_s)$, which went as follows. For each candidate fretboard position, they obtained the expected skewed frequency locations at which to find the inharmonic peaks, and searched the note's spectrum for them using search windows centered at those expected locations. They performed this search for a set number of peaks, all the while cataloguing their deviation from the search window centers. Their motivation was that the expected inharmonicity of the true fretboard position would produce a spectrum with inharmonic peaks that were measurably more coincident with the inharmonic peaks of the unknown note than were any of the imposter strings. They found that, indeed, the theoretical inharmonicity of the fretboard position on which the note was truly plucked yielded the largest number of coincident peaks located inside the spectral search window bounds, hence our name ``partials coincidence tally" for their algorithm. Their work demonstrated the sufficiency of inharmonicity as a discriminating feature.

This method capitalizes further on their trajectory realization. Rather than zooming into fretboard positions' spectra to evaluate their candidacy, we propose characterizing each candidate string by its inharmonicity trajectory and classifying a note as simply belonging to the string whose inharmonicity trajectory evaluated at the note's fundamental best approximates the note's inharmonicity. In other words, our similarity routine is rather some error calculation $A'(\beta,\hat{\beta}_s)$, discussed in detail in the next sections, between the note's measured inharmonicity $\beta$ and each candidate fretboard positions' predicted inharmonicity $\hat{\beta}_s$ for the note. This is an algorithmically simple approach, requiring merely inharmonicity estimation of the unknown note, inharmonicity trends of training guitars' strings, and an error measure. Additionally, this method is governed by the intuitive spatial constraint of inharmonicity trajectory: notes played on a given string can exhibit only a set of inharmonicities, so classifying based on a spatial metric like minimal residual from a regression makes physical sense.

\subsection{Inharmonicity Estimation}
\label{mat}
The first step in our system is estimation of the inharmonicity of an unseen note. The algorithm we employed was the median adjustive trajectories (MAT) method~\cite{hodgkinson2009} proposed by Hodgkinson in 2009, and we present it here. At the core of this method is a recasting of the definition of inharmonicity in terms of any two partials and their indices. We can rearrange equation~\eqref{eq:fk} to express the fundamental $f_0$ in terms of the frequency $f_m$ of the $m$th partial and its index $m$ as
\begin{equation}
\label{mat6}
f_0 = \frac{f_m}{m\sqrt{1+\beta m^2}}.
\end{equation}
Next, we can substitute~\eqref{mat6} back into $f_0$ in equation~\eqref{eq:fk} to obtain
\begin{equation}
\label{mat7}
f_k = \frac{kf_m}{m\sqrt{1+\beta m^2}}\sqrt{1+\beta k^2}.
\end{equation}
Solving for $\beta$ yields
\begin{equation}
\label{mat8}
\beta = \frac{(f_k\frac{m}{k})^2-f_m^2}{k^2f_m^2-m^2(f_k\frac{m}{k})^2},
\end{equation}
which defines the inharmonicity of a note in terms of an arbitrary two partials $k$ and $m$ and their corresponding frequencies $f_k$ and $f_m$.

The MAT algorithm first pre-processes a note with the Complex Spectral Phase Evolution (CSPE) method~\cite{short2006} to refine the subsequent frequency estimates. The inharmonicity routine then begins by locating partials 1 and 2 in this refined spectrum based on a user-input estimate of the fundamental. Searching for these partials is straightforward, since the skew effect of inharmonicity is low for small indices. From these locations,~\eqref{mat8} is used for every combination of the located partials (which for this first iteration, is only one) to obtain an array of $\beta$ estimates. The median of the $\beta$ array is returned as the composite $\beta$ estimate of this iteration, and this median together with the partials' locations are used in~\eqref{mat6} to obtain an array of revised $f_0$ estimates that contains as many elements as partials under consideration. Finally, the median of this $f_0$ array and the median of the $\beta$ array are used in equation~\eqref{eq:fk} to predict the frequency of the next partial for which to search, which is the third in this case. At succeeding iterations, the lengths of these $f_0$ and $\beta$ arrays grow longer (since both the number of partials, and therefore the number of combinations of partials pairs, increases), and the estimates which they contain grow increasingly more accurate. The algorithm is summarized in the flowchart in Figure~\ref{fig:mat-flowchart}, taken from ~\cite{hodgkinson2009}. MAT determines the ideal number of partials for which to search by finding the highest partial index that lies above the average level of the magnitude spectrum, and terminates once this partial index is reached. The result is an efficient and accurate inharmonicity estimation routine. 

We apply the MAT estimation procedure to five consecutive 100ms windows of audio, starting from detection of a note onset. This yields five inharmonicity estimates which we aggregate into one effective result using the frame aggregation process described in Section~\ref{sec:string-classification}.
\begin{figure}[!htbp] 
\label{fig:mat-flowchart}
\centering
\includegraphics[scale=0.9]{mat-flowchart}
\caption{Median-adjustive-trajectory (MAT) method flowchart.}
\end{figure}

\subsection{Log-inharmonicity}
Next in our pipeline, we apply a log transformation to the estimated inharmonicity to make its variation with respect to MIDI pitch linear. This has the advantage of using simpler regression models for classification at later stages. If we reconsider equation~\eqref{eq:beta-traj} in terms of MIDI pitch number $m$ instead of fret number $n$, we obtain
\begin{equation}
\beta(s,m) = \beta(s,m_{os})2^{\frac{m-m_{os}}{6}},
\end{equation}
where $m_{os}$ is the open-string pitch of string $s$. The exponential trajectory is maintained since incrementing both fret number and MIDI pitch number accomplishes an equivalent increase in fundamental frequency. Figure~\ref{fig:beta-v-midi} illustrates the trajectories in terms of MIDI pitches.

\begin{figure}[h] 
\centering
\includegraphics[scale=0.7]{beta-v-midi}
\caption{Inharmonicity estimates for frets 0-12 on strings 1-6 of RWC acoustic guitar 111AG. Note the exponential trajectory as in Fig.~\ref{fig:beta-trajectories-ag} and the increased segregation since notes and their inharmonicities are now plotted against their fundamental instead of their fret number.}
\label{fig:beta-v-midi}
\end{figure}

Now if we consider the log-inharmonicity $\beta_{l}(m)$, we see that
\begin{equation}
\beta_l(m) = \log_2\beta(s,m) = \log_2[\beta(m_{os})2^{\frac{m-m_{os}}{6}}]
\end{equation}
\begin{equation}
\beta_l(m) = \log_2\beta(m_{os}) + \frac{m-m_{os}}{6}
\end{equation}
\begin{equation}
\beta_l(m) = (\log_2\beta(m_{os})-\frac{m_{os}}{6}) + (\frac{1}{6})m,
\end{equation}
where string $s$ has been dropped from the notation since we're looking only at variation along a fixed string. Substituting $w_0$ for $\log_2\beta(m_{os})-\frac{m_{os}}{6}$ and letting $w_1 = \frac{1}{6}$, we obtain the familiar linear form
\begin{equation}
\label{eq:linear-traj}
\beta_l(m) = w_0 + w_1m,
\end{equation}
showing us that the log-inharmonicity trajectory of a given string varies linearly with respect to MIDI pitch. See Figure~\ref{fig:log-beta-v-midi}. This relationship also holds for fret number, but fundamental pitch is a more likely and generic feature to have been estimated, and consequently is more relevant to consider here.

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.7]{log-beta-v-midi}
\caption{Log-inharmonicity estimates for same frets, strings, and guitar as Fig.~\ref{fig:beta-v-midi}}
\label{fig:log-beta-v-midi}
\end{figure}

\subsection{Learning Inharmonicity Trajectories}
We next need to train our system to understand strings' typical log-inharmonicity trajectories. There are two methods we use to do this. The first, which we call \textit{theoretical} trajectory determination, is essentially the method employed by the existing PCT method~\cite{barbanchoi2012}. For each guitar, we collect the six notes labeled with fret 0 (i.e. played open-string) from its recordings, and estimate their inharmonicities. Our line trajectories are then simply those defined in terms of the open-string inharmonicities by the weights in equation~\eqref{eq:linear-traj}. The ``learning" in this case is just the estimation of open-string inharmonicities; the lines themselves are simply extrapolations of the expected theoretical relationship between a string's log-inharmonicity and its pitch.

%The coefficient vector $\mathbf{w}$ is $[w_0, w_1]^T$, with $w_0$ and $w_1$ as defined before. The independent variable vector $\mathbf{x}$ remains $[1, m]^T$, as discussed in the previous paragraph. The predicted inharmonicity $\hat{\beta}$ reduces to an evaluation of the linear function $\beta(m) = \mathbf{w}^T\mathbf{x}$ at the unknown note's fundamental pitch $m$, and it is these evaluations that are used in~\eqref{eq:classify} to classify strings.

The other method we use is linear regressions for obtaining \textit{empirical} log-inharmonicity trajectories. Regressing log-inharmonicities of notes against their pitches could capture a more fitting trajectory than the strictly theoretical trajectories obtained from equation~\eqref{eq:linear-traj}. In this method, we collect all notes with common string labels in our training data, and we perform linear regression of their log-inharmonicities $\beta$ against their fundamental pitches $m$ (in MIDI note number format). We do this for each string label. Let $s \in \{1,2,3,4,5,6\}$ be the string label to which each training note is assigned, and $N_s$ be the number of notes we have belonging to string label $s$. If we let $\mathbf{x}_s^{(i)} = [1, m_s^{(i)}]^T$ represent the $i$th note with fundamental pitch $m^{(i)}$ belonging to string $s$ , and $\beta_s^{(i)}$ be the log-inharmonicity of the $i$th note belonging to string $s$, we can solve
\begin{equation}
\label{lin-reg}
\mathbf{w}_s = \argmin_{\mathbf{w}}{\sum_{i=1}^{N_s}{(\beta^{(i)}_s - \mathbf{w}^T\mathbf{x}^{(i)}_s)^2}}
\end{equation}
which yields the weight vector $\mathbf{w}_s$ that minimizes the sum of squared error between the measured and predicted inharmonicities of the notes belonging to string $s$. Figure~\ref{fig:traj-v-reg} illustrates the differences between the trajectory and regression approaches.

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{traj-v-reg}
\caption{Comparison of string inharmonicity characterization approaches for our recorded Fender Telecaster. Top: \textit{theoretical} trajectories. Note how each line begins precisely at the first marker of each string (i.e. the open-string note) and continues along a trajectory that ignores empirical inharmonicity estimates. String 1 most clearly shows this. Additionally, the trajectories for strings 3 and 4 are effectively co-linear and therefore indiscriminable. Bottom: regressions, or \textit{empirical} trajectories. Each regression was obtained by minimizing the bisquare weighted error of the string's residuals, so the lines more closely follow the empirical inharmonicity scatterplot estimates.}
\label{fig:traj-v-reg}
\end{figure}

Occasional estimates from the inharmonicity measurement routine would be outliers that deviated substantially from the linear trajectory model, and which disproportionately skewed the regressions. For this reason, we rather performed a bisquare weighted linear regression, instead of the standard unweighted regression in~\eqref{lin-reg}, to empirically approximate the log-inharmonicities. Bisquare weighting is a robust linear regression procedure that minimizes a weighted sum of squares that de-emphasizes outliers. The iterative procedure works as follows. First, a regular least squares fit is obtained. Then, the standardized adjusted residuals are computed according to
\begin{equation}
u = \frac{r_i}{Ks\sqrt{1-h_i}},
\end{equation}
where $r_i$ are the plain residuals, $h_i$ are the leverages, $K = 4.685$ is a tuning constant, and $s$ is the robust variance, which is equal to $M/0.6745$ where $M$ is the median absolute deviation of the residuals. Next, the robust weights are computed according to
\begin{equation}
w_i = \begin{cases}
(1-u_i^2)^2, & |u_i| < 1\\
0, & |u_i| \geq 1\\
\end{cases}
\end{equation} 
The process is repeated, iteratively improving the weights until the fit converges~\cite{matlab-robustfit}. The Matlab 'robustfit' routine was used to implement this.

\subsection{String Classification}
\label{sec:string-classification}
The next step is to use these learned string trajectories to classify unknown notes. Since these trajectories tell us how to expect different strings' inharmonicities to vary with respect to pitch, determining the trajectory which best approximates the unseen note's inharmonicity estimate seems a fitting procedure to ascertain the originating string. Specifically, we impose Gaussian distributions centered at the trajectories over each strings' inharmonicity measurements, and evaluate them at unseen notes' inharmonicities to determine the most likely originator strings.

We assign to an unseen note $\mathbf{x}^{(i)} = [1, m^{(i)}]^T$ the string $s$ whose trajectory $\mathbf{w}_s$ most probably produces the note's measured inharmonicity $\beta^{(i)}$. We can think of each inharmonicity trajectory as a trajectory of one-dimensional Gaussians, whose means are the log-inharmonicity values of points on the line, and whose variances are the residuals' variance $\sigma_s^2$. In this manner, the probability distribution that characterizes the inharmonicity trajectory of string $s$ at the $i$th note's fundamental pitch $\mathbf{x}^{(i)} = [1,m^{(i)}]^T$ is defined as the normal distribution $\mathcal{N}(\mu, \sigma^2)$, with mean $\mu = \mathbf{w}_s^T\mathbf{x}^{(i)}$ and variance $\sigma^2 = \sigma_s^2$. Now to classify the $n$th unseen note $\mathbf{x}^{(n)}$, we simply measure its log-inharmonicity $\beta^{(n)}$ and obtain the probability that this log-inharmonicity was observed given each string $s$ according to
\begin{equation}
P(\mathbf{x}_n,\beta^{(n)} | s) = \mathcal{N}(\beta^{(n)} | \mathbf{w}_s^T\mathbf{x}^{(n)},\sigma_s^2),
\end{equation}
and returning the string $s$ that maximizes this probability,
\begin{equation}
s^{(n)} = \argmax_{j}P(\mathbf{x}_n,\beta^{(n)} | j).
\label{eq:string-argmax}
\end{equation}

%We perform probabilistic classification in the following manner. Variances of the residuals $\sigma^2_s$ of each string trajectory $s$ are obtained, and we impose a Gaussian with variance $\sigma^2_s$ centered on the trajectory line. In this fashion, we characterize the spread of measured inharmonicity estimates belonging to each string. Notes are assigned to the strings whose trajectories maximize the likelihood of observing the notes' inharmonicities.

 


%Other decision functions were considered before deciding on residual minimization. We considered using a two-dimensional Euclidean distance metric, but ultimately steered away because it wasn't true to the physical phenomenon -- expected inharmonicity on a fixed string should be a function of only one variable, namely pitch, so incorporating an additional degree of freedom seems inappropriate. Indeed, one can easily imagine situations where the Euclidean distance between a note's inharmonicity estimate and an erroneous string is smaller than the one-dimensional residual between it and its true string, thereby producing an incorrect string classification. 

%We also considered a probabilistic decision function. Because, for linear regression, minimizing the residual sum-of-square error of the data is equivalent to maximizing the likelihood of the data if its assumed the data was generated by Gaussian distributions with means centered at the regression line, we also attempted a probabilistic, or ``soft" residual classifier. Unseen notes were assigned to the line from which it was most probably generated, given the lines' means and variances. The term ``soft" arises from the fact that points are no longer necessarily assigned to the closest line -- the decision is evaluated strictly on probabilistic terms. We ultimately avoided this method, however, because our system was suffering from precision issues. The variances of the robust weighted regressions would consistently be tens or hundreds of orders of magnitude smaller than the distances to some outlier inharmonicities. The system would then incorrectly classify these outliers because of insufficient resolution. A workaround wasn't implemented because this, too, didn't quite fit the mechanism by which these inharmonicities were being produced in reality. Inharmonicity outliers were strictly due to inharmonicity estimation mistakes, not a physical phenomenon in the guitar, so it didn't seem appropriate to incorporate this into our model.

%We thus settled on minimizing the residual error, which is explained here. We take the $n$th note in our test set, characterized by tuple $(m^{(n)},\beta^{(n)})$ where $m^{(n)}$ is its MIDI pitch number and $\beta^{(n)}$ is its inharmonicity. We'd like to transform this into another tuple $(s^{(n)},f^{(n)})$ corresponding to the appropriate string and fret to which this unknown note should be assigned. 

%We straightforwardly assign to $s^{(n)}$ the index $j$ of the linear coefficients $\mathbf{w}_j$ whose predicted inharmonicity $\hat\beta_j$ approximates the measured inharmonicity $\beta^{(n)}$ with least error. More formally, we solve 
%\begin{equation}
%\label{eq:classify}
%s^{(n)} = \argmin_{j}{(\beta^{(n)} - \hat{\beta}_{j})^2}
%\end{equation}
%where $\hat\beta_j$ is the inharmonicity prediction of string $j$, given by
%\begin{equation}
%\hat{\beta}_{j} = \mathbf{w}_{j}^T\mathbf{x}^{(n)}.
%\end{equation}
%Here, $\mathbf{x}^{(n)} = [1, m^{(n)}]^T$ to account for a y-intercept term. Figure~\ref{fig:classify} graphically illustrates this process.

%\begin{figure}[!htbp] 
%\label{fig:classify}
%\centering
%\includegraphics[scale=0.75]{classify}
%\caption{Classifying two new notes A and B using learned log-inharmonicity characterizations. Dashed lines are actual learned regressions for RWC acoustic guitar AG091. Note A with fundamental pitch 62 is assigned to string 3, since its line characterization is closest to A's inharmonicity estimate. The residual of note B, with fundamental pitch 76, is smallest with respect to string 1, so it's assigned to that string under the least-square-error decision of equation~\eqref{eq:classify}.}
%\end{figure}
Another conceivable decision function is classification based on an unseen note's residual distance to the string trajectories. Instead of selecting the string whose trajectory maximizes the probability of inharmonicity observation, one could select the string whose trajectory exhibits minimum residual with the note's inharmonicity measurement. Indeed, this worked well, though experiments comparing this ``hard" classifier versus the aforementioned ``soft" probabilistic classifier found that the probabilistic method performed slightly better, so we used this for our experiments in Chapter~\ref{chap:experiments}.

Occasionally, outlier inharmonicity estimates would throw off this classification scheme. To combat this, we employed frame aggregation, a refinement technique used by~\cite{abesser2012}. In frame aggregation, inharmonicities of multiple audio windows are estimated and classified individually, then the multiple results are aggregated into a compound final decision. In essence, it's a form of averaging that relies on the theoretically low probability that consecutive inharmonicity estimates will all be outliers; the more probable stable estimates will contribute to a more reasonable aggregate string decision. For each frame $r$ in an $R$-frame aggregation of a note, we obtain the string-wise probabilities $\mathbf{p_r} = [p_1,p_2,p_3,p_4,p_5,p_6]^T$ of its log-inharmonicity estimate and all possible strings $s$. Then, we simply sum the probabilities $\mathbf{p_r}$ over all frames and select the string whose aggregate likelihood is maximal:
\begin{equation}
\hat{s} = \argmax_{s\in\{1,2,3,4,5,6\}}\sum_{r=1}^{R} \mathbf{p_r}.
\end{equation}
The result is a slight boost in classification accuracy. For our system, we used 5 abutting frames, each 100ms long, which start at the detected onset of a note. Onsets were obtained through automated detection, the details of which are beyond the scope of this work but are cited for reference~\cite{bello2005,dixon2006}.

\subsection{Tablature Conversion and Refinement}
The final step is transformation of the string classifier output into tablature notation. Because the classifier outputs are string numbers, we still need to infer notes' fret positions. This is a trivial task provided we know the tuning $\mathbf{t} = [m_1, m_2, m_3, m_4, m_5, m_6]^T$ of the unknown guitar, where $m_s$ is the MIDI pitch number of the open note on string $s$. Because we already have an estimate of the $n$th unknown note's MIDI pitch $m^{(n)}$, simply taking the difference between $m^{(n)}$ and its assigned string's open pitch $m_s$ yields the fret on which the note was played; each fret on a guitar increases the string's pitch by one half-step, or equivalently the MIDI pitch number by one integer.

We further refine the fretboard estimates with ``plausbility filtering"~\cite{abesser2012} in which we reject and reassign those string decisions that are implausible given the constraints and assumptions of the data. If our systems assigns to a note a string on which the corresponding fretted position is negative, a mistake has clearly been made since frets can't possibly be lower than 0 (the open-string). Similarly, if our system assigns to a note a string on which the corresponding fret is greater than 12, we know this is an error as well because our guitar recordings feature only frets 0-12. When either of these situations arise, we reject the string classification decision and select the next closest string. If the succeeding string is also implausible, the process is iterated until all six strings have been considered.

\section{Maximum-likelihood Fretboard Position}
\subsection{Motivation}
Our claim with the first method (inharmonicity trajectories) is exploitation of the linear trajectories of log-inharmonicities is helpful for transcription, but in order to objectively evaluate this we need for comparison the results of a baseline classifier which \textit{doesn't} leverage the inharmonicity trajectories. This method, which we're calling simply maximum-likelihood estimation (MLE) of fretboard position, serves as this benchmark requirement. 

\subsection{Inharmonicity Estimation}
As before, the first step in this method is measurement of the inharmonicities of notes. We again use median-adjustive-trajectories (MAT) to estimate this crucial feature. MAT is explained in detail in Section~\ref{mat}.

\subsection{Fretboard Position Inharmonicity Distributions}
Next, we need to capture the inharmonicity distribution of each of the $F = 78$ fretboard positions $f = \{1,2,3..,F\}$ (13 fretting options for each of the 6 strings). For each position, we calculate the median $\tilde{\beta}_f$ and variance $\sigma^2_f$ of its inharmonicities, and use these to parameterize a Gaussian probability density $\mathcal{N}_f(\tilde{\beta}_f,\sigma^2_f)$. The median was chosen to lessen the influence of outliers, and was found to produce slightly better results.

\subsection{Fretboard Position Classification}
Now that each fretboard position $f$ has been characterized as a probability distribution $\mathcal{N}_f$, we can classify an unseen note. With the interpretation of inharmonicity as a random variable, we measure the inharmonicity $\beta^{(i)}$ of the $i$th unseen note $\mathbf{x}=[1,m_0^{(i)}]^T$ with fundamental MIDI pitch $m_0^{(i)}$ and simply evaluate the probability $P_f(\beta^{(i)} | f) = \mathcal{N}_f(\beta^{(i)} | \tilde{\beta}_f,\sigma^2_f)$ of it having been generated by the Gaussian distribution $\mathcal{N}_f$. We narrow our search space $f \in \{1,2,3,...,F\}$ of possible fretboard positions by considering just the potential three positions $f \in \{f_{m,1},f_{m,2},f_{m,3}\}$ that agree with the unseen note's fundamental frequency. For assignment, we select the candidate fretboard position which maximizes the likelihood of the note's measure inharmonicity, just as in equation~\eqref{eq:string-argmax}:
\begin{equation}
\hat{f} = \argmax_{f\in\{f_{m,1},f_{m,2},f_{m,3}\}}P(\beta^{(i)} | f).
\label{eq:string-classification-mle}
\end{equation}
We also perform frame aggregation with this classification method, using the same specifications as discussed in Section~\ref{sec:string-classification}. In summary, five abutting 100ms windows following the onset of a test note are individually analyzed to produce five inharmonicity measurements. Each measurement is used in equation~\eqref{eq:string-classification-mle}, and the individual probabilities for candidate fretboard positions are summed across the five measurements. The position with maximum aggregate probability is selected as the classification output.

\section{Tuning Compensation Feature}
After transforming our data into log-inharmonicity space in the first method, and after estimating the probability distributions of each fretboard positions' inharmonicities in the second method, we can apply a tuning compensation feature step if the test guitar is known to be in alternate-tuning. In this section, we derive the straightforward adjustment and explain how it is incorporated into our system, focusing on the context of the inharmonicity trajectory method.

Though standard tuning is the most common pitch configuration of six-string guitars, there exist numerous other tunings in which performers often play. Aside from altering the musicality of the instrument, alternate tunings complicate the transcription process by introducing uncertainty about the open-string pitches. They distort inharmonicity-based methods, since the tensions $T$ of alternately-tuned guitar strings differ from those in standard-tuned strings, thereby affecting estimation of the inharmonicity. 

Current string classification systems don't address this degree of freedom. PCT~\cite{barbanchoi2012} is technically tuning-invariant, but would require access to recordings of the alternately-tuned test guitar's open strings and so isn't considered a classifier here. Other supervised learning systems~\cite{kehling2014, dittmar2013, abesser2012} are trained and evaluated only on standard-tuning guitars.

A possible approach to augment inharmonicity-based string classifiers with tuning-invariant performance is to simply introduce a scaling factor on the expected inharmonicity, or equivalently an additive factor on the expected log-inharonicity. The fundamental frequency $f_0$ of an ideal vibrating string is related to its tension $T$ according to
\begin{equation}
f_0 = \frac{1}{2L}\sqrt{\frac{T}{\mu}}
\end{equation}
where $L$ is string length and $\mu$ is its density. From this, we can see that
\begin{equation}
T \propto f_0^{2}.
\end{equation}
Recognizing that for a change in fundamental pitch of $\Delta m$ semitones, the equivalent change in frequency is $2^{\frac{\Delta m}{12}}$, we see that the proportional change in tension (with all other factors constant) is
\begin{equation}
T \propto 2^{\frac{\Delta m}{6}}f_0^2.
\end{equation}
The resulting log-inharmonicity of this new open-string note with pitch $m_{os}' = m_{os}+\Delta m$, with original open-string pitch $m_{os}$, is therefore
\begin{equation}
\log_2\beta(m_{os}') = \log_2[ \frac{\pi^3 Q d^4}{64 T l^2}(2^{-\frac{\Delta m}{6}})].
\end{equation}
\begin{equation}
\label{eq:delta-beta}
\log_2\beta(m_{os}') = \log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{\Delta m}{6}.
\end{equation}
Substituting~\eqref{eq:delta-beta} into $w_0$ from equation~\eqref{eq:linear-traj} to obtain the new intercept term $w_0'$, we get
\begin{equation}
w_{0}' = \log_2{\beta}(m_{os}') - \frac{m_{os}'}{6}
\end{equation}
\begin{equation}
w_{0}' = (\log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{\Delta m}{6}) - \frac{m_{os}+\Delta m}{6}
\end{equation}
\begin{equation}
\label{eq:3.17}
w_{0}' = \log_2\frac{\pi^3 Q d^4}{64 T l^2} - \frac{m_{os}}{6} - \frac{\Delta m}{3}
\end{equation}
\begin{equation}
\label{eq:3.18}
w_{0}' = w_0 - \frac{\Delta m}{3}.
\end{equation}
The slope coefficient $w_1 = -\frac{m}{6}$ is independent from ${\Delta m}$, so the affected log-inharmonicity trajectory of this alternately-tuned string is therefore
\begin{equation}
\label{eq:3.19}
\beta'(m) = w_0' + w_1'm
\end{equation}
\begin{equation}
\label{eq:3.20}
\beta'(m) = w_0 + w_1m - \frac{\Delta m}{3}.
\end{equation}
In words, equation~\eqref{eq:3.20} tells us that the effect of a string's alternate tuning is simply addition of a bias term $\frac{-\Delta m}{3}$ to the log-inharmonicity, where $\Delta m$ is the semitone deviation from standard tuning. We should thus be able to compensate our predictive trajectories to arbitrarily-tuned guitars, provided we're given the alternate tuning in which the unseen notes are being performed. As can be seen in Figure~\ref{fig:tuning-eg} when one string on the standard-tuned electric guitar is modified using equation~\eqref{eq:3.20}, the predictive alternate-tuning regression aptly captures the trend of the alternate-tuning inharmonicity. This is a straightforward modification for any inharmonicity-exploiting classifier, and we report our results with this modification in the next chapter.
\begin{figure}[!htbp] 
\label{fig:tuning-eg}
\centering
\includegraphics[scale=0.75]{tuning-eg}
\caption{Effect of alternate tuning on an electric guitar's log-inharmonicity trajectories. The circles are log-inharmonicity estimates of frets 0-12 on string 6 of RWC electric guitar EG131 in standard tuning, and the dashed line is their bisquare-weighted regression. The asterisks are log-inharmonicity estimates for the same frets, string, and guitar recorded two semitones down. The dotted line passing through them is the \textit{predicted} regression obtained by applying equation~\eqref{eq:3.20} to the dashed line.}
\end{figure}


\section{System Overviews}
Below in Figure~\ref{fig:overview}, we summarize the operation of our inharmonicity trajectory system. Inharmonicity estimates are obtained for our training guitars' notes, then we represent these estimates in log-inharmonicity space with respect to MIDI pitch to obtain linear trends. We then check if the evaluation guitars' tunings are non-standard, and if so, we appropriately compensate the weight vectors that define the linear trajectories. Next, we perform classification on test recordings by assigning to unknown notes the strings whose Gaussian log-inharmonicity trajectories maximize the likelihood of the notes' log-inharmonicities. Finally, tablature conversion and refinement is performed with plausibility and constraint filtering.

Our baseline MLE fretboard position classifier is also summarized in Figure~\ref{fig:overviewMLE}. It crucially differs from the inharmonicity trajectory method in that it doesn't exploit any aspect of the trajectory at all; it simply performs MLE using the individual fretboard inharmonicity distributions, providing a relevant benchmark of transcription performance. First, inharmonicity estimates for the training guitar notes are obtained, and normal distributions are fitted to each fretboard positions' inharmonicity variation. Then, if the test guitar's tuning is non-standard, the distributions can be compensated appropriately. Notes from the test guitar are classified by measuring their inharmonicities and, as before, selecting the fretboard positions that maximizes the likelihood of their observation. Tablature refinement is built-in by considering as candidate fretboard positions only those whose fundamental pitches match the unseen note's. 

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.6]{overview}
\caption{System overview of tablature transcription with log-inharmonicity trajectories using probabilistic classification.}
\label{fig:overview}
\end{figure}

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.6]{overviewMLE}
\caption{System overview of tablature transcription with MLE fretboard position classification.}
\label{fig:overviewMLE}
\end{figure}

\noindent
\chapter{Experiments and Results}
\label{chap:experiments}
\section{Benchmark RWC Evaluation}
%We used subsets of the Real World Corpus' Music Instrument Database (RWC-MDB-I)~\cite{goto2003}, subsets of the Fraunhofer Institute for Digital Media Technology's Semantic Music Technologies guitar dataset (IDMT-SMT)~\cite{asdf}, and personal recordings of an electric guitar to conduct our experiments. 

We used subsets of the Real World Corpus Music Instrument Database (RWC-MDB-I)~\cite{goto2003} and personal recordings of an electric and acoustic guitar to conduct our experiments. 

Our RWC subset comprised nine guitar recordings (three classical, three acoustic, and three electric), each of which were performed twelve times exhibiting various permutations of particular musicality parameters (playing style, dynamic level, pickup selection). For our work, we used only six of the twelve recordings for each of the electric guitars, as the discarded ones featured musicality parameters like vibrato and palm-muting that weren't featured in the classical and acoustic sets. The performances themselves were simply clean, isolated, monophonic enumeration of every fret (from open-string to 12th fret) on every string (from string 6 to string 1), constituting 78 total note plucks per audio file. The resolution is 16 bits per sample, at 44.1kHz. Labels of strings and frets are thus obtained by their location of occurrence in the recording. The classical guitars recorded were a Stafford, a Sakurai Kohno Professional-J, and a Yuichi Imai YJ-II; the three acoustic guitars captured were an Ovation, a Yamaha APX, and Yairi WY1; the electric guitars featured were a Fender Stratocaster, an Aria PE, and an Ibanez Artcore.

%The IDMT-SMT guitar dataset focuses on electric guitars, and boasts standardized licks performed across its sample instruments. The bit depth here is 24, with sampling rate equal to 44.1kHz. The subset we used featured six short licks (each between 10 and 30 notes), each of which were captured on three electric guitars: an Aristedes 010, a Fender Stratocaster, and a Gibson Les Paul. Transcriptions which included string and fret labels were saved as accompanying .xml files.

We also recorded a Fender Telecaster (electric guitar) and a Takamine G-series (acoustic guitar) at 16 bits and 44.1kHz in the same vein as the RWC database: string and fret enumeration, totaling 78 notes per recording. We captured various tunings: standard, ``DADGAD" (in which the open pitches of strings 6 through 1 are given by the respective letters in name), ``WSU" (whole-step up), and ``WSD" (whole-step down). The Fender, which was recorded directly into our audio interface, was performed with a thick plectrum at center pickup orientation and at similar moderate dynamic levels throughout. We recorded the Takamine in an isolated acoustic chamber with a large diaphragm condenser microphone (an MXL 990) placed 12 inches away from fret 12, and we also played it with a thick plectrum and at constant dynamic levels. Analog-to-digital hardware included a Focusrite Saffire Pro 24 and a 2011 Macbook Pro running Logic. No other musicality parameters were captured; we focused on tuning variance here.

We conduct experiments on all nine RWC guitars at three levels of context, which comprise the headings of Table~\ref{tab:error-results-RWC}: guitar-specific, guitar-averaged, and guitar-independent. For guitar-specific trials, we train and test the system on the same guitar using three-fold cross-validation with test folds that are one-third of the guitar's recordings. The folds' results are averaged together for composite transcription errors. For guitar-averaged trials, we train the system on all recordings of the two guitars in the same class as the test guitar, as well as a two-thirds training portion of the test guitar, and then evaluate on the remaining third of the test guitar recordings. Again, three-fold cross-validation is used to iterate through the portions of the test recordings. For guitar-independent trials, the system never sees any of the test guitar's recordings before evaluation. We train the system on two of the guitars in the same class as the test guitar, and test on all recordings of the remaining one. In all cases, Table~\ref{tab:error-results-RWC} reports error probabilities, as this was the transcription metric used in~\cite{barbanchoi2012}.

In the ``MLE" columns of each heading in Table~\ref{tab:error-results-RWC}, we report transcription results using the baseline MLE classifier we developed. To review, Gaussian distributions were fit to the training data's inharmonicity measurements of every fretboard position. Then, notes were classified by selecting the fretboard positions that most probably produced their measured inharmonicities, according to the learned Gaussian likelihoods.

We next report string identification results (taken from~\cite{barbanchoi2012}) using the PCT method under the ``PCT" columns of Table~\ref{tab:error-results-RWC}. As discussed in Chapter~\ref{chap:lit-review}, this method uses estimates of the open-string inharmonicities of the training audio's guitar(s) to predict \textit{expected} inharmonicities of an unknown note's candidate fretboard positions. The degree of similarity between the unknown note's measured inharmonicity and that of the candidate fretboard positions is assessed by tallying their coincident partials. The candidate fretboard position that produces the highest tally is returned as the transcribed tablature. Note that no results for the ``Guitar-independent" column exist for this method, as no results were reported in~\cite{barbanchoi2012} for this scenario.

%Our second experiment was comparison of our classification method (residual error of each string's trajectory) to the benchmark classification method (partials coincidence tally, or PCT). This experiment used \textit{theoretical} inharmonicity trajectories. For each guitar in our RWC subset, we obtained log-inharmonicity estimates of its open-string notes from two-thirds of its recordings, then performed string classification on the remaining third using residual error minimization on the theoretical trajectories, as discussed in Chapter~\ref{chap:method}. We repeated this twice with different folds for three-fold cross validation, and averaged each fold's probability of an erroneous transcription to produce composite error probabilities. These are reported under the ``theo." column of the ``Guitar-specific $\beta$'s" heading in Table~\ref{tab:error-results-RWC}. This experiment was also repeated for more general inharmonicity contexts: we estimated log-inharmonicity for the open-string notes of two-thirds of the test guitar's recordings, in addition to all of the open-string notes belonging to the other guitars in the same class. As before, we performed three-fold cross validation on the test guitar's recordings, and report error probabilities under the ``theo." column of the ``Guitar-averaged $\beta$'s" heading. This heading is named as such because the effect of learning from all three guitars is an ``averaging" of their open-string notes' inharmonicities.

Finally, we evaluate our trajectory classification method, reporting error probaiblities using empirical trajectories (i.e. regressions) under the ``Regr." columns of Table~\ref{tab:error-results-RWC}. We performed bisquare weighted regression of the training guitars' log-inharmonicity estimates against their pitches. String classification was performed on the test notes by selecting the regressions whose imposed Gaussian PDFs maximized the likelihood of the notes' inharmonicities, as discussed in Chapter~\ref{chap:method}. We also evaluated our \textit{theoretical} trajectory method, in which regressions aren't computed but likelihood maximization is still used for classification. However, error probabilities were only slightly better than those of our \textit{empirical} trajectory (i.e. regression) method, so we omitted them from Table~\ref{tab:error-results-RWC} for conciseness.

%Our second experiment was an evaluation of our classification method using \textit{empirical} inharmonicity trajectories, or inharmonicity regressions. For each of the training guitars  String classification was performed on the remaining third using residual error minimization on the regressions, as discussed in Chapter~\ref{chap:method}. We repeat this using the same number of folds as the first experiment, and report error probabilities for each guitar under the ``empi." column of the ``Guitar-specific $\beta$'s" heading in~\ref{tab:error-results-RWC}. Again, we also evaluate regressions learned from all three guitars in the same class as the test guitar's, using the same cross-validation configuration. These results are reported under the ``empi." column of the ``Guitar-averaged $\beta$'s".

We point the reader's attention to the ``Overall..." rows in Table~\ref{tab:error-results-RWC} to highlight the central results of this work. For the columns under the ``Guitar-specific" column, PCT excels for every guitar. Clearly, tallying the coincident partials using inharmonicity estimates from the exact guitar on which tests are conducted outperforms the MLE baseline and our regression classification method. Though for the ``Guitar-averaged" scenario, both our baseline MLE and our regression method outperform PCT in overall average performance. And finally, we see that our regression-based scheme outperforms our baseline scheme in overall performance, suggesting that there is indeed merit in leveraging the structure of the inharmonicity trajectories for improved diverse guitar tablature transcription.

%\begin{table}[!htbp]
%\begin{center}
%\begin{tabular} {||c||c|c|c||c|c|c||}
%\hline
%\multicolumn{7}{|c|}{\bf{Transcription Error Probabilities}} \\
%\hline
% & \multicolumn{3}{|c|}{Guitar-specific $\beta$'s} & \multicolumn{3}{|c|}{Guitar-averaged $\beta$'s}\\
%\hline
%Guitar & PCT~\cite{barbanchoi2012} & theo. & empi. & PCT~\cite{barbanchoi2012} & theo. & empi.\\
%\hline
%\hline
%Classical CG091 & 0.04 & 0.02 & 0.04 & 0.09 & 0.02 & 0.04\\
%\hline
%Classical CG092 & 0.00 & 0.08 & 0.08 & 0.01 & 0.08 &  0.08\\
%\hline
%Classical CG093 & 0.01 & 0.07 & 0.08 & 0.00 & 0.08 & 0.09\\
%\hline
%Overall CG: & \bf{0.02} & \bf{0.06} & \bf{0.07} & \bf{0.03} & \bf{0.06} & \bf{0.07}\\
%\hline
%\hline
%Acoustic AG111 & 0.00 & 0.01 & 0.04 & 0.06 & 0.00 & 0.04 \\
%\hline
%Acoustic AG112 & 0.00 & 0.02 & 0.03 & 0.22 & 0.11 & 0.06 \\
%\hline
%Acoustic AG113  & 0.00 & 0.03 & 0.06 & 0.04 & 0.04 & 0.08\\
%\hline
%Overall AG: & \bf{0.00} & \bf{0.02} & \bf{0.04} & \bf{0.11} & \bf{0.05} & \bf{0.06} \\
%\hline
%\hline
%Electric EG131 & 0.00 & 0.20 & 0.22 & 0.20 & 0.19 & 0.21 \\
%\hline
%Electric EG132 & 0.01 & 0.20 & 0.16 & 0.23 & 0.19 & 0.23 \\
%\hline
%Electric EG133 & 0.00 & 0.12 & 0.07 & 0.32 & 0.16  & 0.24 \\
%\hline
%Overall EG: & \bf{0.00} & \bf{0.17} & \bf{0.15} & \bf{0.25} & \bf{0.18} & \bf{0.23}\\
%\hline
%\end{tabular}
%\caption{Error probability comparisons between PCT, residual minimization with theoretical trajectories, and residual minimization with empirical trajectories. PCT results are taken from~\cite{barbanchoi2012}; we've excluded error due to $f_0$ estimation in their results.}
%\label{tab:error-results-RWC}
%\end{center}
%\end{table}

\begin{table}[!htbp]
\begin{center}
\begin{tabular} {||c||c|c|c||c|c|c||c|c|c||}
\hline
\multicolumn{10}{||c||}{\bf{Transcription Error Probabilities}} \\
\hline
 & \multicolumn{3}{|c||}{Guitar-specific} & \multicolumn{3}{|c||}{Guitar-averaged}& \multicolumn{3}{|c||}{Guitar-independent}\\
\hline
Guitar & MLE & PCT & Regr. & MLE & PCT & Regr. & MLE & PCT & Regr.\\
\hline
\hline
Classical CG091 & 0.02 & 0.04 & 0.03 & 0.13 & 0.09 & 0.03 & 0.14 & -- & 0.03 \\
\hline
Classical CG092 & 0.15 & 0.00 & 0.06 & 0.16 & 0.01 & 0.06 & 0.13 & -- & 0.03 \\
\hline
Classical CG093 & 0.08 & 0.01 & 0.04 & 0.14 & 0.00 & 0.06 & 0.14 & -- & 0.06 \\
\hline
Overall CG: & \bf{0.08} & \bf{0.02}  & \bf{0.04} & \bf{0.14} & \bf{0.03} & \bf{0.05} & \bf{0.14} & -- & \bf{0.05}\\
\hline
\hline
Acoustic AG111 & 0.00 & 0.00 & 0.03 & 0.01 & 0.06 & 0.03 & 0.02 & -- & 0.03 \\
\hline
Acoustic AG112 & 0.02 & 0.00 & 0.03 & 0.03 & 0.22 & 0.06 & 0.13 & -- & 0.13 \\
\hline
Acoustic AG113  & 0.03 & 0.00 & 0.04 & 0.06 & 0.04 & 0.06 & 0.10 & -- & 0.07\\
\hline
Overall AG: & \bf{0.02} & \bf{0.00} & \bf{0.03} & \bf{0.03} & \bf{0.11} & \bf{0.05} & \bf{0.08} & -- & \bf{0.08}\\
\hline
\hline
Electric EG131 & 0.22 & 0.00 & 0.18 & 0.21 & 0.20 & 0.18 & 0.24 & -- & 0.23\\
\hline
Electric EG132 & 0.15 & 0.01 & 0.10 & 0.15 & 0.23 & 0.17 & 0.17 & -- & 0.21\\
\hline
Electric EG133 & 0.06 & 0.00 & 0.09 & 0.20 & 0.32  & 0.19 & 0.22 & -- & 0.27 \\
\hline
Overall EG: & \bf{0.14} & \bf{0.00} & \bf{0.12} & \bf{0.19} & \bf{0.25} & \bf{0.18} & \bf{0.21} & -- & \bf{0.24}\\
\hline
\hline
Overall Average: & \bf{0.08} & \bf{0.01} & \bf{0.06} & \bf{0.12} & \bf{0.13} & \bf{0.09} & \bf{0.14} & -- & \bf{0.12}\\
\hline
\end{tabular}
\caption{Error probability comparisons between PCT, the baseline maximum likelihood estimate classifier MLE, residual minimization with theoretical trajectories, and residual minimization with empirical trajectories. PCT results are taken from~\cite{barbanchoi2012}; we've excluded error due to $f_0$ estimation in their results.}
\label{tab:error-results-RWC}
\end{center}
\end{table}

%\begin{table}[!htbp]
%\begin{center}
%\begin{tabular} {||c||c|c||c|c||}
%\hline
%\multicolumn{5}{|c|}{\bf{Transcription F1-scores}} \\
%\hline
% & \multicolumn{2}{|c|}{Guitar-specific $\beta$'s} & \multicolumn{2}{|c|}{Guitar-averaged $\beta$'s}\\
%\hline
%Guitar & theo. & empi. & theo. & empi.\\
%\hline
%\hline
%Classical CG091 & 0.98 & 0.96 & 0.98 & 0.96\\
%\hline
%Classical CG092  & 0.93 & 0.92 &  0.93 &  0.92\\
%\hline
%Classical CG093  & 0.92 & 0.92 &  0.91 & 0.91\\
%\hline
%Overall CG:  & \bf{0.94} & \bf{0.93} & \bf{0.94} & \bf{0.93}\\
%\hline
%\hline
%Acoustic AG111 & 0.99 & 0.97 &  1.00 & 0.96 \\
%\hline
%Acoustic AG112 & 0.97 & 0.97 &  0.87 & 0.94 \\
%\hline
%Acoustic AG113 & 0.97 & 0.94 & 0.96 & 0.92\\
%\hline
%Overall AG: & \bf{0.98} & \bf{0.96} & \bf{0.94} & \bf{0.94} \\
%\hline
%\hline
%Electric EG131  & 0.78 & 0.78 & 0.79 & 0.78 \\
%\hline
%Electric EG132 & 0.76 & 0.83 & 0.76 & 0.74 \\
%\hline
%Electric EG133 & 0.89 & 0.93 & 0.79  & 0.76 \\
%\hline
%Overall EG: & \bf{0.81} & \bf{0.85} & \bf{0.78} & \bf{0.76}\\
%\hline
%\end{tabular}
%\caption{F1-scores for theoretical and empirical log-inharmonicity trajectory classification. PCT column removed because F1-scores weren't reported in~\cite{barbanchoi2012}.}
%\label{tab:f-results-RWC}
%\end{center}
%\end{table}

Also displayed in Tables~\ref{tab:cg-str-f},~\ref{tab:ag-str-f}, and~\ref{tab:eg-str-f} are the \textit{string-wise} F1-scores using our regression-based classifier for the ``Guitar-averaged" scenario. The F1-score is the harmonic mean of precision and recall, and is a commonly used classification metric elsewhere, so we use it here and wherever comparison with~\cite{barbanchoi2012} is absent. These tables' left-hand columns denote the string number of the rows' F1-scores, and the right-hand columns are the row-wise average of the three guitars in the class. We present this information to highlight the individual strings' contributions to the overall transcription accuracy.

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{||c|c|c|c|c||}
\hline
\multicolumn{5}{||c||}{\bf{Transcription F1-scores}} \\
\hline
String No. & CG1 & CG2 & CG3 & Overall CG \\
\hline
1 & 0.96 & 0.93 & 0.96 & 0.95\\
\hline
2 & 0.92 & 0.90 & 0.91 & 0.91\\
\hline
3 & 1.00 & 0.95 & 0.96 & 0.97\\
\hline
4 & 0.96 & 0.93 & 0.91 & 0.93\\
\hline
5 & 1.00 & 0.96 & 0.93 & 0.96 \\
\hline
6 & 1.00 & 0.98 & 1.00 & 0.99\\ 
\hline
\hline
\end{tabular}
\caption{String-wise F1-scores for the RWC classical guitars, using regression-residual minimization for the ``Guitar-averaged" scenario.} 
\label{tab:cg-str-f}
\end{center}
\end{table}

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{||c||c|c|c|c||}
\hline
\multicolumn{5}{||c||}{\bf{Transcription F1-scores}} \\
\hline
String No. & AG1 & AG2 & AG3 & Overall AG \\
\hline
1 &  0.96 & 0.96 & 0.96 & 0.96 \\
\hline
2 & 0.94 & 0.80 & 0.87 &  0.87\\
\hline
3 & 0.91 & 0.89 & 0.85 & 0.88\\
\hline
4 & 1.00 & 1.00 & 0.99 &  1.00 \\
\hline
5 & 1.00 & 1.00 & 0.99 &  1.00 \\
\hline
6 & 1.00 & 1.00 & 1.00 & 1.00 \\ 
\hline
\hline
\end{tabular}
\caption{String-wise F1-scores for the RWC acoustic guitars, using regression-residual minimization for the ``Guitar-averaged" scenario.} 
\label{tab:ag-str-f}
\end{center}
\end{table}

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{||c||c|c|c|c||}
\hline
\multicolumn{5}{||c||}{\bf{Transcription F1-scores}} \\
\hline
String No. & EG1 & EG2 & EG3 & Overall EG\\
\hline
1 & 0.96 & 0.94 & 0.96 & 0.95 \\
\hline
2 & 0.94 & 0.95 & 0.72 & 0.87\\
\hline
3 & 0.47 & 0.42 & 0.42 &  0.44\\
\hline
4 & 0.66 & 0.72 & 0.79 &  0.72\\
\hline
5 & 0.85 & 0.92 & 1.00 &  0.92 \\
\hline
6 & 0.96 & 0.98 & 1.00 &  0.98 \\ 
\hline
\hline
\end{tabular}
\caption{String-wise F1-scores for the RWC electric guitars, using regression-residual minimization for the ``Guitar-averaged" scenario.} 
\label{tab:eg-str-f}
\end{center}
\end{table}

\section{Tuning Compensation}
For the next experiment, we learned inharmonicity regressions from all recordings of the standard-tuned RWC electric and acoustic guitar recordings, and predicted string classes on our alternate-tuned Fender Telecaster and Takamine G-series recordings, respectively. We evaluated string-wise F1-score performance both with and without the tuning compensation factor in our system discussed in Chapter~\ref{chap:method}. Tuning compensation results are denoted by the ``comp." column, and original results without compensation are denoted by the ``orig." column. See Table~\ref{tab:resultsTune}. For reference, we also report transcription performance on our personal guitars in standard tuning, in the second column from the left.

In standard tuning, strings 6 through 1 are tuned to E2, A2, D3, G3, B3, and E4. In so-called ``DADGAD" tuning, the open-string pitches are made to be D2, A2, D3, G3, A3, and D4 as these open pitches concatenated together form its name. Whole-step up and whole-step down are tunings in which each of the standard-tuned strings are respectively incremented or decremented by two semitones. For whole-step down, this results in D2, G2, C3, F3, A3, and D4. For whole-step up, we have F\#2, B2, E3, A3, C\#4, and F\#4.

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{||c||c||c|c||c|c||c|c||}
\hline
\multicolumn{8}{|c|}{\bf{Alternate-tuning Transcription F1-scores (Electric)}} \\
\hline
& Standard & \multicolumn{2}{|c|}{``DADGAD"} & \multicolumn{2}{|c|}{``WSU"} & \multicolumn{2}{|c|}{``WSD"} \\
\hline
String No. & orig. & orig. & comp. & orig. & comp. & orig. & comp. \\
\hline
1 & 0.96 & 0.75 & 0.96 & 0.83 & 0.96 & 0.80 & 0.96 \\
\hline
2 & 0.96 & 0.78 & 0.92 & 0.58 & 0.96 & 0.83 & 0.96\\
\hline
3 & 0.50 & 0.40 & 0.43 & 0.56 & 0.50 & 0.29 & 0.50\\
\hline
4 & 0.76 & 0.76 & 0.74 & 0.23 & 0.76 & 0.65 & 0.76 \\
\hline
5 & 1.00 & 1.00 & 0.87 & 0.33 & 1.00 & 0.85 & 1.00 \\
\hline
6 & 1.00 & 0.92 & 1.00 & 0.88 & 1.00 & 0.85 & 1.00\\ 
\hline
\hline
Overall: & 0.86 & 0.77 & 0.82 & 0.57 & 0.86 & 0.71 & 0.86\\
\hline
\end{tabular}
\caption{String-wise F1-scores for our electric Fender Telecaster at various tunings. Regressions trained on all recordings of all three RWC electric guitars. WSU: whole-step up (from standard); WSD: whole-step down (from standard); orig.: no tuning compensation used in this trial; comp.: tuning compensation used in this trial. The ``Overall" row displays average F1-score of each column.} 
\label{tab:resultsTune}
\end{center}
\end{table}

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{||c||c||c|c||c|c||c|c||}
\hline
\multicolumn{8}{|c|}{\bf{Alternate-tuning Transcription F1-scores (Acoustic)}} \\
\hline
& Standard & \multicolumn{2}{|c|}{``DADGAD"} & \multicolumn{2}{|c|}{``WSU"} & \multicolumn{2}{|c|}{``WSD"} \\
\hline
String No. & orig. & orig. & comp. & orig. & comp. & orig. & comp. \\
\hline
1 & 0.96 & 0.82 & 0.92 & 0.92 & 0.96 & 0.82 & 0.96 \\
\hline
2 & 0.82 & 0.70 & 0.63 & 0.45 & 0.82 & 0.61 & 0.92\\
\hline
3 & 0.96 & 0.84 & 1.00 & 0.71 & 0.96 & 0.00 & 0.96\\
\hline
4 & 0.87 & 0.79 & 0.79 & 0.81 & 0.87 & 0.18 & 0.93 \\
\hline
5 & 1.00 & 1.00 & 1.00 & 0.77 & 1.00 & 0.61 & 1.00 \\
\hline
6 & 1.00 & 0.92 & 1.00 & 0.83 & 1.00 & 0.81 & 1.00\\ 
\hline
\hline
Overall: & 0.93 & 0.84 & 0.89 & 0.75 & 0.93 & 0.50 & 0.96\\
\hline
\end{tabular}
\caption{String-wise F1-scores for our acoustic Takamine G-series at various tunings. Regressions trained on all recordings of all three RWC acoustic guitars. WSU: whole-step up (from standard); WSD: whole-step down (from standard); orig.: no tuning compensation used in this trial; comp.: tuning compensation used in this trial. The ``Overall" row displays average F1-score of each column.} 
\label{tab:results-ag-tune}
\end{center}
\end{table}

\noindent
\chapter{Discussion}
\label{chap:discussion}
In this chapter, we begin with an analysis of the previous chapter's results, followed by consideration of factors that potentially limit the significance of our findings. We close with a discussion about future work.

\section{Analysis}
Comparison of our regression-based likelihood classifier (``Regr." columns) to our baseline likelihood classifier (``MLE" columns) in Table~\ref{tab:error-results-RWC} reveals that leveraging the linear structure of log-inharmonicities indeed provides an advantage for transcription accuracy. In all evaluation scenarios (i.e., headings in Table~\ref{tab:error-results-RWC}), the overall-average error probability of our regression method outperforms the baseline by about two points.

Our regression approach exhibited noteworthy behavior. It performed worse than PCT for the case when training and testing were both performed exclusively on one guitar whose open-string log-inharmonicities we had access to (the ``Guitar-specific" heading in Table~\ref{tab:error-results-RWC}, comparing columns ``PCT" and ``Regr."), but we achieve lower overall-averaged error probabilities when the training set diversifies and multiple guitars' log-inharmoncities are considered (the ``Guitar-averaged" heading). It seems this is a strength of our system; PCT shines with reliable specific test-guitar inharmonicity information, but degrades faster and more variably when the reliability of that information decreases. Its worst case performance degradation (for EG133) is from zero error to 25\% error as the guitar-specific inharmonicity context is lost, but contrastingly the error probability of our regression classification worsens only marginally at its worst case (also for EG133), degrading by only 6 percentage points. PCT performance on guitar-averaged inharmonicities is also more erratic, with error probability variances for all three guitar classes about one order of magnitude greater than our regressions classification. Also, though PCT results for the ``Guitar-independent" scenario aren't available, it's likely that our approaches exceed its performance by an even greater margin if we extrapolate its error probabilities using its rate of degradation from ``Guitar-specific" to ``Guitar-averaged". It appears that the trajectory-leveraging classification approach trades peak performance capability for more stable results that are also more robust to different guitars' inharmonicities.



%A possible reason PCT is superior when guitar-specific inharmonicities are available is the flexibility of its partial coincidence tallying routine. The authors of~\cite{barbanchoi2012} note that when searching for coincident peaks, they found that varying the higher partial for which to search would change the algorithm's certainty about its string decision. Different regions of partials in the spectrum wou

%Interestingly, classification using regressions on inharmonicity estimates collected from a single guitar generally performed worse than classification with the theoretical trajectories obtained from the open-string inharmonicity estimates of the same guitar. (Compare columns ``theo." and ``empi." under the ``Guitar-specific $\beta$'s" heading). This was contrary to our hypothesis that the regressions would more accurately capture the empirical trajectory than would the theoretical one.

The alternate-tuning experiments validated our tuning compensation feature nicely. We see in Tables~\ref{tab:resultsTune} and~\ref{tab:results-ag-tune} that the general effect of appropriately scaling the inharmonicity regressions is to restore transcription F1-scores towards best-case standard-tuned performance.

Further insight into the error probabilities in Tables~\ref{tab:error-results-RWC} is gained by considering the string-wise F1-scores in Tables~\ref{tab:cg-str-f},~\ref{tab:ag-str-f}, and~\ref{tab:eg-str-f}. We see that across all guitars, strings 2, 3, and 4 generally exhibit lower F1-scores. Strings 1, 5, and 6, contrastingly, generally exhibit exceptional performance, so we know that any poor results in the ``Regr." column in Table~\ref{tab:error-results-RWC} are likely to have stemmed from poor accuracy on these problematic ``middle" strings that are physically bordered on the instrument by higher and lower strings. We thus see a flaw in our regression-based inharmonicity similarity approach: because of its inherently spatial classification decision, this system exhibits poor performance on strings whose trajectories share overlapping regions in inharmonicity-pitch space.

It's unclear why performance for electric guitars is invariably worse than for classicals and acoustics. To investigate, we visually compared our string-wise inharmonicity regressions of all guitars in each guitar class. As can be seen in Figures~\ref{fig:cg-traj-comp},~\ref{fig:ag-traj-comp}, and~\ref{fig:eg-traj-comp}, the classical and acoustic guitars' inharmonicity regressions remain fairly stable from guitar to guitar, whereas those of the electric guitars clearly exhibit more variability, making lkelihood-based classification more difficult. Electric guitar regressions belonging to strings 3 and 4 overlap considerably, which is a behavior not absent in the classicals (strings 1 and 4, 2 and 5, 3 and 6) and acoustics (strings 2 and 3) with some strings but which manifests in less error due to their regressions' more consistent placements than those of their electric counterparts. Indeed, the electric guitars' confusion matrices support this, and one which highlights this error trend is shown in Table~\ref{tab:cf-eg}. 
%Example confusion matrices for the CGs and AGs are also shown; the overlapping error doesn't seem to affect the CGs and AGs nearly as much, since their regression variability is much smaller.

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{traj-compare-cg}
\caption{Superimposed string-wise regressions of all three RWC classical guitars. Top: strings 1, 2, and 3. Bottom: strings 4, 5, and 6. Plots were separated for visibility, and axes scales were kept identical for comparison. Observe the consistent intercept and slope despite varying guitar (with the only exception being string 6).}
\label{fig:cg-traj-comp}
\end{figure}

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{traj-compare-ag}
\caption{Superimposed string-wise regressions of all three RWC acoustic guitars. Top: strings 1 and 2. Bottom: strings 3, 4, 5, and 6. Plots were separated for visibility, and axes scales were kept identical for comparison. Observe the consistent intercept and slope despite varying guitar.}
\label{fig:ag-traj-comp}
\end{figure}

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{traj-compare-eg}
\caption{Superimposed string-wise regressions of all three RWC electric guitars. Top: strings 1, 2, and 3. Bottom: strings 4, 5, and 6. Plots were separated for visibility, and axes scales were kept identical for comparison. Observe the \textit{inconsistent} intercept and slope. In particular, note the overlap between one of string 3's regressions (dashed red) and string 2's regressions. Also observe the overlapping slope and intercept ranges for strings 3 and 4 across the two plots.}
\label{fig:eg-traj-comp}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
& \multicolumn{6}{|c|}{Classification} \\
\hline
Truth &1	&2	&3	&4	&5	&6\\
\hline
\hline
1	&\bf{1.00}	& 0	& 0	& 0	&0	& 0 \\ 
\hline
2	&0	& \bf{0.77}	& 0.23	& 0	&0	& 0 \\
\hline
3	&0	& 0.08	& \bf{0.62}	&0.38	& 0	& 0 \\ 
\hline
4	&0	& 0	& 0.23	&\bf{0.46}	& 0.31	& 0 \\
\hline
5	&0	& 0.08	& 0	&0	& \bf{0.77}	& 0.15\\ 
\hline
6	&0	& 0	& 0	&0	&0	& \bf{1.00} \\
\hline
\end{tabular}
\caption{Confusion matrix for one classification trial with EG131. Strings 3 and 4, as they are here, were frequently misclassified because of their regressions' poor discriminability.} 
\label{tab:cf-eg}
\end{center}
\end{table}

Further analysis revealed that inconsistent inharmonicity estimates were responsible for the electric guitars' regression variability. We examined the quality of fit of string regressions obtained from all recordings of each of the 3 guitars from each of the 3 types, and found that the classicals' and acoustics' range of determination coefficients was substantially smaller than the electrics'. Highest and lowest quality regressions for CG091, AG111, and EG131 are shown in Figure~\ref{fig:best-worst-r2} as an example of this; CG091's and AG111's best and worst strings' $r^2$ coefficients differ by only about one-hundredth, but the $r^2$ difference for EG131 is an enormous $0.87$.

\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{best-worst-r2}
\caption{Best and worst bisquare log-inharmonicity regression fits, as measured by $r^2$, for representative guitars from each of the three classes. Top row: classical. Middle row: acoustic. Bottom row: electric. Left and right columns are best and worst fits, respectively.}
\label{fig:best-worst-r2}
\end{figure}

Speculating why this might be the case, we wondered if the transduction of the EG pickups somehow played a role in impeding reliable inharmonicity estimation -- either through their transfer functions, possible noise introduction, or some non-linear characteristic. If this were the case, it would follow that higher-intensity performances would elicit this impeding phenomenon to a greater extent and produce measurably more variable inharmonicity estimates than would lower-intensity performances. Since one of the RWC musicality parameters that varies for each featured guitar is dynamic level of the performance, we were able to separate higher- and lower-intensity performances of the same guitar and analyze this interaction between degree of pickup transduction and inharmonicity estimation variability. For each guitar, and for each string, we collected note performances by common dynamic levels (piano, mezzo, and forte) and regressed their log-inharmonicities against their fundamentals, as usual. We then plotted statistics of the log-inharmonicities' residuals against their played performance intensity. Figure~\ref{fig:eg1-string-dyn} illustrates these statistics for EG131; other electric guitars' boxplots were omitted because their statistics were similar.
\begin{figure}[!htbp] 
\centering
\includegraphics[scale=0.75]{eg1-string-dyn}
\caption{Boxplots of log-inharmonicity regression residuals for EG131 by dynamic level. Group 1: piano, Group 2: mezzo, Group 3: forte. Red center horizontal lines are medians; top and bottom edges of blue boxes represent upper and lower quartiles; whiskers depict full range of data excluding outliers; red plus signs denote outliers. Vertical axes not constant throughout plots; we're highlighting relative distributions of the boxplots here.}
\label{fig:eg1-string-dyn}
\end{figure} 
We expected that variability would increase with dynamics, but the data didn't support this hypothesis. Interquartile ranges didn't exhibit any clear trend with respect to performance intensity, nor was the magnitude of their variation noteworthy. The cause of the inharmonicity unreliability is therefore probably linked to some spectral characteristic of the electric guitars, e.g. considerably more non-tonal peaks which the MAT algorithm erroneously selects as partials, or some aspect of the pickup's frequency response that's throwing off the MAT algorithm, etc.

\section{Limitations}
It's important to highlight factors that potentially limit the significance of our claims. Our system currently operates only on clean (i.e. no effects processing), isolated, monophonic guitar recordings, which are obviously not representative of the majority of guitar audio. Multiple layers of effects processing are commonly used by performers of the instrument to achieve distinctive, creative sounds. Guitarists often play in concert with other musicians, producing complex soundscapes instead of being featured in isolation. Moreover, guitar passages are rarely in their entirety strictly monophonic. These are large obstacles that separate our RWC subset from actual guitar performances, and thus the implications of our transcription results should be considerably tempered. Noteworthy though, is the extent to which other teams have overcome some of these factors. Certain tablature transcription systems~\cite{barbanchoi2012,abesser2012,dittmar2013,kehling2014} have implemented polyphonic capabilities into their systems with good success, so polyphony is perhaps a smaller barrier than the others. As part of a multipitch estimation system in~\cite{yazawa2013}, Yazawa deduced tablature from RWC recordings that featured guitar passages playing over backing tracks, though the tablature accuracy is not reported.

Additionally, we assumed perfect fundamental pitch estimation in this work. Our decision to simply use pitch labels of the RWC recordings stemmed from convenience and scope; the focus of this work was simply evaluation of a specific approach to examining notes' inharmonicities for tablature transcription, so not concerning ourselves with the reliability of a pitch detection routine was a luxury that expedited this thesis. That being said, it should be acknowledged that the reliability of one's pitch estimator is a sure factor in the translation of this work's results to the real world.

Furthermore, the MAT inharmonicity estimation routine we used was different from Barbancho's. The credibility of the comparison between our results and theirs is thus somewhat compromised. It's quite possible that any performance gains observed in our implementation are due in part to potentially superior inharmonicity extraction. This is, unfortunately, a factor we didn't control for, and if time had allowed we should have replicated Barbancho's system and substituted their inharmonicity estimation step with MAT to produce results that were as comparable as possible.

\section{Future Work}
A potential area for run-time performance enhancement for tablature transcription systems is the use of musical context. Musical passages on the guitar tend to exhibit continuity; it's usually the case that a riff is played in a localized area of the fretboard, for example, instead of jumping around the length of the guitar neck. Transcription systems could apply these performance-borne simplifications to their output to potentially improve accuracy, say, when a stray note is transcribed a plausible yet questionable twelve frets away from the rest of those in the riff.

A measure of objective confidence in the transcription is required to do this. Without it, there'd be no way to determine whether it was the stray note or the majority of the riff which was in fact correctly transcribed. We investigated the amenability of our log-inharmonicity residual-minimizing classification scheme to reliable confidence measurements, and found encouraging results. Figure~\ref{fig:p-correct-res} displays the probability, for each of the nine RWC guitars, of our system producing a correct fretboard transcription given increasingly wide windows in which we consider a note's inharmonicity estimate to have fallen. The inverse trend between residual window width and correctness probability shows that we can reliably infer some degree of confidence in a note's transcription given its distance from its classified line. This can be leveraged in a post-processing stage to further refine tablature with isolated erroneous, yet plausible, transcriptions, and should be implemented in future systems.

Another dimension we didn't consider was varying the regression polynomial. It would be interesting to see how differently, if at all, the system would perform if the log inharmonicity transformation was forgone and an exponential fit was explored instead. This should also be investigated in future inharmonicity-based transcription research.

\begin{figure}[!htbp] 
\centering
\includegraphics[angle=90,scale=0.65]{p-correct-res}
\caption{Solid black lines and left vertical axis: probability of correctly transcribing a note given the residual width under which the note's log-inharmonicity falls. Dashed orange lines and right vertical axis: CDF of the log-inharmonicity residuals, so one can get a sense of the proportion of data represented by ticks in the horizontal axis.}
\label{fig:p-correct-res}
\end{figure} 

%The basic transcription results obtained in the previous chapter are modest; we achieved overall transcription accuracies of 43\%, 71\%, and 62\% for the RWC classical, acoustic, and electric guitars respectively. These accuracies don't tell the whole story, though. Closer inspection of Tables~\ref{tab:resultsRWC} and~\ref{tab:resultsTune} reveals that performance is heavily string-dependent. We can achieve a 0.93 F1-measure for string 6 (the electric guitar), but our worst-case F1-measure for string 3 is a disappointing 0.07 (also for the electric guitar). This is likely due to the somewhat overlapping inharmonicity trajectories of the inner strings, i.e. string 3 through 5 mainly, which is discussed further below when we consider factors that may have weakened system quality. This disparity between string-wise F1-scores suggests that in its current state, our system is more appropriate for tablature transcription of the ``outer" strings, i.e. mainly strings 1, 2, and 6.

%Comparing these results to Barbancho's~\cite{barbanchoi2012} state-of-the-art inharmonicity-based transcription system, we see that average transcription accuracies of 98.3\%, 100\%, and 99.6\% are attainable. Though if we consider their performance when using inharmonicity coefficients averaged by guitar type (instead of those of each specific guitar), their performance slightly degrades to 96\%, 89.3\%, and 74\% mean accuracy. Comparison of our system with these poorer accuracies is actually fair, since we also use inharmonicity regressions obtained from averages over all guitars in each type.

%Still, our system's performance is lacking, and various factors could have contributed to this. Inconsistent inharmonicity estimation was likely the largest obstacle to greater performance. Sometimes our estimation routine would easily locate the inharmonic partials in the spectrum to produce a reliable estimate. But often it wouldn't, locating spurious spectral peaks that would corrupt the partials deviations calculation, which would subsequently corrupt the polynomial fit from which the inharmonicity estimate was derived. This variability in our system was somewhat mitigated when performing the inharmonicity regressions across many guitar recordings, effectively averaging it out, but no such averaging was in place for estimating inharmonicity of a single note. In this way, the sequential note-by-note transcription task challenged our system. 

%Additionally, the inharmonicity trajectories of the inner strings often intersected. This was a problem for our system, since our string classification mechanism was simply distance-minimization between the unknown note's inharmonicity and the observed inharmonicity trajectories. Notes with estimated inharmonicities residing close to that intersection would yield unpredictable assignments to either string. Clearly, even with flawless inharmonicity estimation, our transcription accuracy would have been limited.

%Results from the tuning compensation experiment are encouraging. Despite the inharmonicity estimation variability, we see that appropriately scaling the inharmonicity trajectories improves F1-score across the board by 17.5\% on average. Additionally, the individual string-wise F1-scores for all three tunings are non-decreasing after application of this scaling factor. In some cases, e.g. string 3 on the ``WSD" tuning, drastic F1-score improvement from 0.11 to 0.45 occurs. This small experiment confirms this feature as a potential scope-widening addition to inharmonicity-based transcription systems, and justifies further investigation by other researchers with higher-accuracy implementations.

\noindent
\chapter{Summary}
\label{chap:conclusion}
We began by introducing the guitar and an essential component of its musicality: the overlapping pitch ranges of its strings. We related this to tablature and its ability to uniquely specify fretboard positions for musical passages, then highlighted its popularity and its tedious manual annotation process. This led us to introduce automatic transcription systems and our goals for this work as an extension to the topic, which we reviewed in the following chapter.

We paid special attention to the partial coincidence tally (PCT) transcription method introduced by Barbancho in~\cite{barbanchoi2012}, which exploited only one feature known as inharmonicity in development of a successful system using only audio. We explained how inharmonicity affects the degree of upward skew of a note's partials, then discussed the intuition behind its discriminative power before introducing our novel approach.

After reiterating one of Barbancho's key realizations -- that inharmonicity along a given string was deterministic -- we showed that the logarithm of these trajectories was linear with respect to MIDI pitch. We consequently proposed characterizing guitar strings as their log-inharmonicity trajectories and performing classification (and therefore implicitly tablature transcription) by assigning to unknown notes the strings whose lines best approximated the notes' log-inharmonicities. Lastly, we derived the effect of a tuning change on these lines, and arrived at a general tuning-compensation feature applicable to any inharmonicity-based system.

Next, we reported results from experiments on a subset of the RWC music instruments database and a couple personal guitar recordings. Benchmark comparisons against the PCT method using theoretical and empirical inharmonicity trajectories revealed that our method outperformed the state-of-the-art for acoustics and electrics when trained on guitars that included others in addition to the test guitar. Residual-minimization on inharmonicity trajectories therefore leverages the inharmonicity feature in a more generalizable fashion. We also validated our inharmonicity compensation feature, reporting accuracy increases on personal recordings of alternately-tuned electric and acoustic guitars.

A cursory investigation of the cause for the poorer electric guitar transcription performance revealed that inharmonicity estimation for the instrument is less reliable, and the electric guitars' pickups aren't to blame; it must rather be spectral nuances that confuse the inharmonicity estimation algorithm. We closed with limitations and future scope, cautioning against misinformed application of our reported results to real-world guitar scenarios, and highlighting musical context exploitation and different regression polynomials as potential areas for elaboration.

In conclusion, characterizing guitar strings as log-inharmonicity trajectories against pitch and classifying notes by minimizing their trajectory residuals is an effective approach to tablature transcription. Most notably, it outperforms the benchmark inharmonicity-based method on most of the RWC guitar classes when trained on reference guitars that include others in addition to the test guitar. Furthermore, adjusting inharmonicity estimates with the compensation factor introduced in this work is effective for improving transcription accuracy on alternately-tuned guitars.

%We introduced an additional inharmonicity-based approach to guitar string classification and tablature transcription, based on characterizing guitar strings' inharmonicity trajectories with linear regressions against pitch. Classification is performed by assigning to an unknown note the index of the trajectory which best explains the note's inharmonicity. We reported transcription F1-score performance of $0.43$, $0.71$, and $0.62$ for classical, acoustic, and electric guitars in the RWC dataset. 

%A tuning compensation feature was also proposed and analyzed. We derived that a deviation from standard tuning could be accounted for in our regressions with a scaling factor on the deviant strings' trajectories. To test this, we recorded an electric guitar in standard, ``dadgad", whole-step up, and whole-step down tunings, and performed classification both with and without tuning compensation. With our modification, our F1-scores improved by 17.5\% on average for the three tunings we investigated.

%Our system falls short of other inharmonicity-based transcription methods, though we suspect this is largely due to poor internal inharmonicity estimation. Encouragingly, our tuning compensation feature improved performance on alternate-tunings, and should be investigated by other researchers with higher-accuracy transcription systems. Future work should explore other polynomial regressions that more closely match theoretical inharmonicity trajectories. Additionally, performance assessment on polyphonic inputs is requisite for a more thorough appraisal of this system's merit.

%\appendix
%\include{appendix}

\backmatter

%\renewcommand{\baselinestretch}{1.0}\normalsize

% By default \bibsection is \chapter*, but we really want this to show
% up in the table of contents and pdf bookmarks.
\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography'' header and the actual list of references}
\bibliography{mybib} %your bib file
\bibliographystyle{plainnat}


\end{document}
